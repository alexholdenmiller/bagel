{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7174763,"sourceType":"datasetVersion","datasetId":4145920},{"sourceId":7178052,"sourceType":"datasetVersion","datasetId":4148410}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom abc import abstractmethod","metadata":{"_uuid":"6b4779e1-7124-469e-bd5e-d21c3965dd7b","_cell_guid":"d2325094-7319-4a96-942c-2c5704cc8770","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:20.832504Z","iopub.execute_input":"2023-12-11T21:00:20.832798Z","iopub.status.idle":"2023-12-11T21:00:26.879134Z","shell.execute_reply.started":"2023-12-11T21:00:20.832773Z","shell.execute_reply":"2023-12-11T21:00:26.878219Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List, Callable, Union, Any, TypeVar, Tuple\n# from torch import tensor as Tensor\n\nTensor = TypeVar('torch.tensor')","metadata":{"_uuid":"8cd34558-33b5-44df-b988-feec2b932dd5","_cell_guid":"a739e128-7f89-4776-8f7f-13ab91dca6ee","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:26.880634Z","iopub.execute_input":"2023-12-11T21:00:26.881027Z","iopub.status.idle":"2023-12-11T21:00:26.887492Z","shell.execute_reply.started":"2023-12-11T21:00:26.880997Z","shell.execute_reply":"2023-12-11T21:00:26.886824Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaseVAE(nn.Module):\n    \n    def __init__(self) -> None:\n        super(BaseVAE, self).__init__()\n\n    def encode(self, input: Tensor) -> List[Tensor]:\n        raise NotImplementedError\n\n    def decode(self, input: Tensor) -> Any:\n        raise NotImplementedError\n\n    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n        raise NotImplementedError\n\n    def generate(self, x: Tensor, **kwargs) -> Tensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def forward(self, *inputs: Tensor) -> Tensor:\n        pass\n\n    @abstractmethod\n    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n        pass","metadata":{"_uuid":"d87f34e8-9899-4ecf-9381-1891e8218084","_cell_guid":"cf529191-1551-4f7f-a0dc-26d9f2e5b74f","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:26.888649Z","iopub.execute_input":"2023-12-11T21:00:26.888993Z","iopub.status.idle":"2023-12-11T21:00:26.902322Z","shell.execute_reply.started":"2023-12-11T21:00:26.888956Z","shell.execute_reply":"2023-12-11T21:00:26.901472Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VectorQuantizer(nn.Module):\n    \"\"\"\n    Reference:\n    [1] https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py\n    \"\"\"\n    def __init__(self,\n                 num_embeddings: int,\n                 embedding_dim: int,\n                 beta: float = 0.25,\n                 use_ema = False,\n                 decay=0.99,\n                 epsilon=1e-5):\n        super(VectorQuantizer, self).__init__()\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.beta = beta\n\n        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n        \n        self.use_ema = use_ema\n        \n        if self.use_ema:\n            self.embedding.weight.data.normal_()\n            self.register_buffer('_ema_cluster_size', torch.zeros(self.num_embeddings))\n            self.ema_w = nn.Parameter(torch.Tensor(self.num_embeddings, self.embedding_dim))\n            self.ema_w.data.normal_()\n\n            self.decay = decay\n            self.epsilon = epsilon\n        else:\n            self.embedding.weight.data.uniform_(-1 / self.num_embeddings, 1 / self.num_embeddings)\n        \n\n    def forward(self, latents: Tensor) -> Tensor:\n        latents = latents.permute(0, 2, 3, 1).contiguous()  # [B x D x H x W] -> [B x H x W x D]\n        latents_shape = latents.shape\n        flat_latents = latents.view(-1, self.embedding_dim)  # [BHW x D]\n\n        # Compute L2 distance between latents and embedding weights\n        dist = torch.sum(flat_latents ** 2, dim=1, keepdim=True) + \\\n               torch.sum(self.embedding.weight ** 2, dim=1) - \\\n               2 * torch.matmul(flat_latents, self.embedding.weight.t())  # [BHW x K]\n\n        # Get the encoding that has the min distance\n        encoding_inds = torch.argmin(dist, dim=1).unsqueeze(1)  # [BHW, 1]\n\n        # Convert to one-hot encodings\n        device = latents.device\n        encoding_one_hot = torch.zeros(encoding_inds.size(0), self.num_embeddings, device=device)\n        encoding_one_hot.scatter_(1, encoding_inds, 1)  # [BHW x K]\n        \n        # Use EMA to update the embedding vectors\n        if self.use_ema:\n            self._ema_cluster_size = self._ema_cluster_size * self.decay + \\\n                (1 - self.decay) * torch.sum(encoding_one_hot, 0)\n    \n            n = torch.sum(self._ema_cluster_size.data)\n            self._ema_cluster_size = (\n                (self._ema_cluster_size + self.epsilon)\n                / (n + self.num_embeddings * self.epsilon) * n\n            )\n            \n            dw = torch.matmul(encoding_one_hot.t(), flat_latents)\n            self.ema_w = nn.Parameter(self.ema_w * self.decay + (1 - self.decay) * dw)\n        \n            self.embedding.weight = nn.Parameter(self.ema_w / self._ema_cluster_size.unsqueeze(1))\n        \n        # Quantize the latents\n        quantized_latents = torch.matmul(encoding_one_hot, self.embedding.weight)  # [BHW, D]\n        quantized_latents = quantized_latents.view(latents_shape)  # [B x H x W x D]\n\n        # Compute the VQ Losses\n        commitment_loss = F.mse_loss(quantized_latents.detach(), latents)\n        \n        if self.use_ema:\n            vq_loss = commitment_loss * self.beta\n        else:\n            embedding_loss = F.mse_loss(quantized_latents, latents.detach())\n            vq_loss = commitment_loss * self.beta + embedding_loss\n\n        # Add the residue back to the latents\n        quantized_latents = latents + (quantized_latents - latents).detach()\n\n        avg_probs = torch.mean(encoding_one_hot, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n        \n        return vq_loss, perplexity, quantized_latents.permute(0,3,1,2).contiguous(), encoding_one_hot","metadata":{"_uuid":"396d6fc3-ea02-4213-950d-bd9bb1f2093b","_cell_guid":"908f64c5-42df-4df1-8343-bf83c7c76d9f","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:26.905136Z","iopub.execute_input":"2023-12-11T21:00:26.905415Z","iopub.status.idle":"2023-12-11T21:00:26.924844Z","shell.execute_reply.started":"2023-12-11T21:00:26.905391Z","shell.execute_reply":"2023-12-11T21:00:26.924024Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualLayer(nn.Module):\n\n    def __init__(self,\n                 in_channels: int,\n                 out_channels: int):\n        super(ResidualLayer, self).__init__()\n        self.resblock = nn.Sequential(nn.Conv2d(in_channels, out_channels,\n                                                kernel_size=3, padding=1, bias=False),\n                                      nn.ReLU(True),\n                                      nn.Conv2d(out_channels, out_channels,\n                                                kernel_size=1, bias=False))\n\n    def forward(self, input: Tensor) -> Tensor:\n        return input + self.resblock(input)","metadata":{"_uuid":"55a5f7fa-93d0-45c4-a03f-0a232483b956","_cell_guid":"15568924-13d7-4634-aaf7-42f570f2ad6d","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:26.925915Z","iopub.execute_input":"2023-12-11T21:00:26.926195Z","iopub.status.idle":"2023-12-11T21:00:26.944358Z","shell.execute_reply.started":"2023-12-11T21:00:26.926170Z","shell.execute_reply":"2023-12-11T21:00:26.943476Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VQVAE(BaseVAE):\n\n    def __init__(self,\n                 in_channels: int,\n                 embedding_dim: int,\n                 num_embeddings: int,\n                 use_ema: True,\n                 hidden_dims: List = None,\n                 beta: float = 0.25,\n                 img_size: int = 32,\n                 **kwargs) -> None:\n        super(VQVAE, self).__init__()\n\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        self.img_size = img_size\n        self.beta = beta\n\n        modules = []\n        if hidden_dims is None:\n            hidden_dims = [128, 256]\n\n        # Build Encoder\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels=h_dim,\n                              kernel_size=4, stride=2, padding=1),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n\n        modules.append(\n            nn.Sequential(\n                nn.Conv2d(in_channels, in_channels,\n                          kernel_size=3, stride=1, padding=1),\n                nn.LeakyReLU())\n        )\n\n        for _ in range(6):\n            modules.append(ResidualLayer(in_channels, in_channels))\n        modules.append(nn.LeakyReLU())\n\n        modules.append(\n            nn.Sequential(\n                nn.Conv2d(in_channels, embedding_dim,\n                          kernel_size=1, stride=1),\n                nn.LeakyReLU())\n        )\n\n        self.encoder = nn.Sequential(*modules)\n\n        self.vq_layer = VectorQuantizer(num_embeddings,\n                                        embedding_dim,\n                                        self.beta,\n                                        use_ema)\n\n        # Build Decoder\n        modules = []\n        modules.append(\n            nn.Sequential(\n                nn.Conv2d(embedding_dim,\n                          hidden_dims[-1],\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.LeakyReLU())\n        )\n\n        for _ in range(6):\n            modules.append(ResidualLayer(hidden_dims[-1], hidden_dims[-1]))\n\n        modules.append(nn.LeakyReLU())\n\n        hidden_dims.reverse()\n\n        for i in range(len(hidden_dims) - 1):\n            modules.append(\n                nn.Sequential(\n                    nn.ConvTranspose2d(hidden_dims[i],\n                                       hidden_dims[i + 1],\n                                       kernel_size=4,\n                                       stride=2,\n                                       padding=1),\n                    nn.LeakyReLU())\n            )\n\n        modules.append(\n            nn.Sequential(\n                nn.ConvTranspose2d(hidden_dims[-1],\n                                   out_channels=3,\n                                   kernel_size=4,\n                                   stride=2, padding=1),\n                nn.Tanh()))\n\n        self.decoder = nn.Sequential(*modules)\n\n    def encode(self, input: Tensor) -> List[Tensor]:\n        \"\"\"\n        Encodes the input by passing through the encoder network\n        and returns the latent codes.\n        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n        :return: (Tensor) List of latent codes\n        \"\"\"\n        result = self.encoder(input)\n        return [result]\n\n    def decode(self, z: Tensor) -> Tensor:\n        \"\"\"\n        Maps the given latent codes\n        onto the image space.\n        :param z: (Tensor) [B x D x H x W]\n        :return: (Tensor) [B x C x H x W]\n        \"\"\"\n\n        result = self.decoder(z)\n        return result\n    \n    def vq(self, encoding: List[Tensor]): \n        return self.vq_layer(encoding)\n\n    def forward(self, inputs: Tensor, **kwargs) -> List[Tensor]:\n        encoding = self.encode(inputs)[0]\n        vqloss, perplexity, quantized_inputs, encoding_one_hot = self.vq(encoding)\n        reconstructed = self.decode(quantized_inputs)\n        return reconstructed, vqloss, perplexity\n\n    def sample(self,\n               num_samples: int,\n               current_device: Union[int, str], **kwargs) -> Tensor:\n        raise Warning('VQVAE sampler is not implemented.')\n\n    def generate(self, x: Tensor, **kwargs) -> Tensor:\n        \"\"\"\n        Given an input image x, returns the reconstructed image\n        :param x: (Tensor) [B x C x H x W]\n        :return: (Tensor) [B x C x H x W]\n        \"\"\"\n\n        return self.forward(x)[0]","metadata":{"_uuid":"119cc4e5-1b23-4cb8-9028-8fdd22aa6c2b","_cell_guid":"869850c3-fa32-4ef0-8d70-a4cb139f5b0d","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:26.945793Z","iopub.execute_input":"2023-12-11T21:00:26.946493Z","iopub.status.idle":"2023-12-11T21:00:26.966969Z","shell.execute_reply.started":"2023-12-11T21:00:26.946464Z","shell.execute_reply":"2023-12-11T21:00:26.966221Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torchvision.transforms as transforms\n\nimage_size = 32\n# Define a transform to normalize the data\n# Load the CIFAR-10 dataset\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.Resize(image_size),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\n# Download the training set\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n\n# Create a DataLoader for the training set\nbatch_size = 64\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n\n# Download the test set\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\n# Create a DataLoader for the test set\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)","metadata":{"_uuid":"482c7733-8783-4148-9ce1-0993daa28c3c","_cell_guid":"27deaca4-f015-48cb-a902-315dfaa3b099","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:26.968050Z","iopub.execute_input":"2023-12-11T21:00:26.968331Z","iopub.status.idle":"2023-12-11T21:00:34.704176Z","shell.execute_reply.started":"2023-12-11T21:00:26.968308Z","shell.execute_reply":"2023-12-11T21:00:34.703261Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Function to unnormalize and display an image\ndef imshow(img):\n    img = img / 2 + 0.5  # Unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# Define a transform to reverse the normalization\nreverse_transform = transforms.Compose([\n    transforms.Normalize((-0.5, -0.5, -0.5), (1.0, 1.0, 1.0)),\n    transforms.ToPILImage()\n])\n\n# Get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# Show images\nimshow(torchvision.utils.make_grid(images))","metadata":{"_uuid":"79d4446e-16e0-4127-8ccc-9a9124509e25","_cell_guid":"c233058c-2bde-440d-a237-a477b0624971","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:34.705293Z","iopub.execute_input":"2023-12-11T21:00:34.705634Z","iopub.status.idle":"2023-12-11T21:00:35.360938Z","shell.execute_reply.started":"2023-12-11T21:00:34.705606Z","shell.execute_reply":"2023-12-11T21:00:35.359984Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"_uuid":"2300db16-dbcc-4c5e-8efe-4a74d1c90a32","_cell_guid":"16d57706-5473-4df3-b3a9-a95f83202c4f","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:35.362692Z","iopub.execute_input":"2023-12-11T21:00:35.363423Z","iopub.status.idle":"2023-12-11T21:00:35.428304Z","shell.execute_reply.started":"2023-12-11T21:00:35.363388Z","shell.execute_reply":"2023-12-11T21:00:35.427237Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_variance = np.var(trainset.data / 255.0)","metadata":{"_uuid":"494d5da8-25fb-4b27-9902-aa46d97f5107","_cell_guid":"9f803445-50a8-4fd5-9e43-affad4535b70","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T18:54:54.606580Z","iopub.execute_input":"2023-12-11T18:54:54.607505Z","iopub.status.idle":"2023-12-11T18:54:55.707864Z","shell.execute_reply.started":"2023-12-11T18:54:54.607456Z","shell.execute_reply":"2023-12-11T18:54:55.706954Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb","metadata":{"_uuid":"4f7fde55-d6ed-4893-9a0c-c97d48a26180","_cell_guid":"0afb113f-204e-45da-bc70-8a8a7f26852f","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T18:54:56.659787Z","iopub.execute_input":"2023-12-11T18:54:56.660573Z","iopub.status.idle":"2023-12-11T18:54:57.058641Z","shell.execute_reply.started":"2023-12-11T18:54:56.660538Z","shell.execute_reply":"2023-12-11T18:54:57.057875Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vqvae_cfg = {\n    'epochs':25,\n    'in_channels':3,\n    'num_embed':512,\n    'embed_dim':64,\n    'lr':3e-4,\n    'use_ema':True\n}\n\nwandb.init(project='bagel',group='vqvae',config=vqvae_cfg)","metadata":{"_uuid":"1b426b48-0ed7-408a-9b6e-578009bac848","_cell_guid":"dacad66d-a5ed-42d9-a4a5-ba5a30358234","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.optim as optim\n# Instantiate the VQ-VAE model\nmodel = VQVAE(in_channels=3, num_embeddings=512, embedding_dim=64, use_ema=True).to(device)\nmodel.train()\n\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=3e-4)\n\ntrain_losses = []\nrecon_losses = []\nvq_losses = []\nppls = []\nLOSS = float('inf')\n\n# Training loop\nnum_epochs = 25\n\nfor epoch in tqdm(range(num_epochs)):\n    for data in trainloader:\n        inputs, _ = data\n        inputs = inputs.to(device)\n        optimizer.zero_grad()\n\n        # Forward pass\n        reconstructed, vq_loss, _ = model(inputs)\n\n        # Compute the loss\n        recon_loss = criterion(reconstructed, inputs) \n        loss = recon_loss + vq_loss\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n    train_losses.append(loss.item())\n    recon_losses.append(recon_loss.item())\n    vq_losses.append(vq_loss.item())\n    \n    # Save the trained model\n    if loss < LOSS:\n        torch.save({'state_dict': model.state_dict(), 'epoch': epoch, 'loss': loss}, 'vq_vae_model_ema.pth')\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n    \n    wandb.log({'vqvae_loss':loss.item(), 'epoch':epoch})","metadata":{"_uuid":"610f360b-85ac-4d91-93bf-1ebf881136c7","_cell_guid":"458a91c1-49dd-4324-bdf6-7b32ab7192f9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the trained VQ-VAE as an image tokenizer\n# You can use the encoder part of the model to obtain codes for input images\n# Example:\n# \n# model.eval()","metadata":{"_uuid":"b2a094b5-aa4a-48ff-a8c4-f77d767d5ecb","_cell_guid":"3535351d-6635-407c-86f3-feec24c65851","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"c109f05c-6152-4027-9a06-ad0079aa1469","_cell_guid":"01dd700c-23f9-46dd-ab39-7b54e04175fe","trusted":true}},{"cell_type":"code","source":"# vqvae_model.eval()\n# with torch.no_grad():\n#     for data in trainloader:\n#         inputs, _ = data\n#         inputs = inputs.to(device)\n#         encoded_inputs = vqvae_model.encode(inputs)[0]\n#         for param in vqvae_model.parameters():\n#             print(param.requires_grad)\n#         _, _, quantized_inputs, _ = vqvae_model.vq(encoded_inputs)\n#     #     flattened_codes = quantized_inputs.view(inputs.size(0), -1, inputs.dim)\n#         flattened_codes = quantized_inputs.flatten(2).transpose(1, 2)\n#         vq_dim = flattened_codes.size(1)\n#         project_vs = nn.Linear(vq_dim, edim).to(device)\n#         flattened_codes = project_vs(flattened_codes)","metadata":{"_uuid":"22903855-4433-4d64-ad3a-4ec159f56c0f","_cell_guid":"d37ffa10-ec2b-478e-928a-450494f07d12","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"89aee19e-c0da-48b7-94c9-d5b4bec324c9","_cell_guid":"519d0d43-e36e-48ba-afcf-302c87591ebe","trusted":true}},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n      # TODO\n\n        super(PatchEmbedding, self).__init__()\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.embed_dim = embed_dim\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        self.proj = nn.Conv2d(self.in_channels, \n                            self.embed_dim, \n                            kernel_size=self.patch_size, \n                            stride=self.patch_size\n                           )\n        self.norm = nn.LayerNorm(self.embed_dim)\n\n    def forward(self, x):\n        # TODO\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n\n        return x","metadata":{"_uuid":"5e73b706-f57d-4c00-87a9-d85f65891f0e","_cell_guid":"4b873b16-fcf6-4fa3-89e7-1e4c5b83ab32","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:46.515487Z","iopub.execute_input":"2023-12-11T21:00:46.515880Z","iopub.status.idle":"2023-12-11T21:00:46.525597Z","shell.execute_reply.started":"2023-12-11T21:00:46.515839Z","shell.execute_reply":"2023-12-11T21:00:46.524598Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        # TODO\n        super(MultiHeadSelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        self.scale = self.head_dim ** -0.5\n        self.qkv = nn.Linear(self.embed_dim, self.embed_dim * 3)\n        self.qk_norm = False\n        self.use_activation = False\n        self.activation = nn.ReLU() if self.use_activation else nn.Identity()\n        self.q_norm = nn.LayerNorm(self.head_dim) if self.qk_norm else nn.Identity()\n        self.k_norm = nn.LayerNorm(self.head_dim) if self.qk_norm else nn.Identity()\n        self.attn_dropout = nn.Dropout(0.1)\n        self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.proj_dropout = nn.Dropout(0)\n\n    def forward(self, x):\n        # TODO\n        batch_si, seq_len, emb_dim = x.shape\n        qkv = self.qkv(x).reshape(batch_si, seq_len, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n        q, k = self.q_norm(q), self.k_norm(k)\n\n        q = q * self.scale\n        attention = q @ k.transpose(-2, -1)\n        attention = attention.softmax(dim=-1)\n        attention = self.attn_dropout(attention)\n\n        z = attention @ v\n        z = z.transpose(1, 2).reshape(batch_si, seq_len, emb_dim)\n        z = self.proj(z)\n#         z = self.proj_dropout(z)\n        return z","metadata":{"_uuid":"702a69fe-dc96-4c9b-9c6e-be6f5e65370a","_cell_guid":"5f449695-a872-425c-a0c7-471ca0a31143","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:48.050844Z","iopub.execute_input":"2023-12-11T21:00:48.051818Z","iopub.status.idle":"2023-12-11T21:00:48.063045Z","shell.execute_reply.started":"2023-12-11T21:00:48.051780Z","shell.execute_reply":"2023-12-11T21:00:48.062016Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n        # TODO\n        super(TransformerBlock, self).__init__()\n        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n        self.attention_norm = nn.LayerNorm(embed_dim)\n        self.mlp = nn.Sequential(nn.Linear(embed_dim, mlp_dim),\n                                  nn.GELU(),\n                                  nn.Dropout(dropout),\n                                  nn.Linear(mlp_dim, embed_dim)\n                                  # nn.Dropout(dropout)\n        )\n        self.mlp_norm = nn.LayerNorm(embed_dim)\n\n\n    def forward(self, x):\n        # TODO\n        res = x\n        x = self.attention_norm(x)\n        x = self.attention(x)\n        x = x + res # residual connection\n        res = x\n        x = self.mlp_norm(x)\n        x = self.mlp(x)\n        x = x + res\n        return x","metadata":{"_uuid":"5f33f7cf-7c33-4c9f-be22-3dafcfec489a","_cell_guid":"fd8625a8-e26b-40c0-aed7-487a10c4fbea","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:51.223660Z","iopub.execute_input":"2023-12-11T21:00:51.224061Z","iopub.status.idle":"2023-12-11T21:00:51.232258Z","shell.execute_reply.started":"2023-12-11T21:00:51.224031Z","shell.execute_reply":"2023-12-11T21:00:51.231393Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VitWithVQVAE(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout, mode, vq_dim=None):\n        # TODO\n        super(VitWithVQVAE, self).__init__()\n        self.mode = mode\n        if self.mode == 'patch':\n            self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n            self.embed_len = self.patch_embed.num_patches + 1\n        if self.mode == 'vqvae':\n            self.conv_dilate = 1\n            self.conv_kernel = 4\n            self.conv_stride = 4\n            self.vqvae = vqvae_model\n            self.vq_dim = vq_dim\n            self.project_vs = nn.Linear(vq_dim, embed_dim)\n            self.embed_len = int(((image_size - self.conv_dilate * (self.conv_kernel - 1) - 1) / self.conv_stride + 1) ** 2) + 1  # max seqlen\n        if self.mode == \"conv\":\n            self.conv_dilate = 1\n            self.conv_kernel = 5\n            self.conv_stride = 3\n            self.conv2d = nn.Conv2d(in_channels,\n                                    embed_dim * 1,\n                                    5,\n                                    groups=1,\n                                    stride=3,\n                                    dilation=1)\n            self.proj_feats = nn.Linear(embed_dim * 1, embed_dim)\n            self.feat_norm = nn.LayerNorm(embed_dim)\n            self.embed_len = int(((image_size - self.conv_dilate * (self.conv_kernel - 1) - 1) / self.conv_stride + 1) ** 2) + 1  # max seqlen\n\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.embed_len, embed_dim))\n        self.dropout = nn.Dropout(dropout)\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for i in range(num_layers)\n        ])\n        self.norm = nn.LayerNorm(embed_dim)\n        self.cls_head = nn.Sequential(nn.Linear(embed_dim, embed_dim//2),\n                                nn.GELU(),\n                                nn.Dropout(dropout),\n                                nn.Linear(embed_dim // 2, num_classes),\n                                # nn.Dropout(dropout)     \n                                )                           \n\n    def forward(self, x):\n        # TODO\n        if self.mode == \"patch\":\n            x = self.patch_embed(x)\n#             print(\"patch_embed\",x.shape)\n        if self.mode == \"vqvae\":\n            self.vqvae.eval()\n            with torch.no_grad():\n                z = self.vqvae.encode(x)[0]\n                _, _, quantized_z, _ = vqvae_model.vq(x)\n#                 print(quantized_z.shape)\n                flattened_z = quantized_z.flatten(2).transpose(1, 2)\n#                 print(flattened_z.shape)\n                x = self.project_vs(flattened_z)\n#                 x = self.dropout(x)\n#                 print(\"proj vs shape\", x.shape)\n\n        if self.mode == \"conv\":\n            x = self.conv2d(x)\n#             print(x.shape)\n            vqvae_model.eval()\n            with torch.no_grad():\n                _, _, quantized_z, _ = vqvae_model.vq(x)\n#             print(quantized_z.shape)\n            flattened_z = quantized_z.flatten(2).transpose(1, 2)\n#             print(flattened_z.shape)\n            x = self.proj_feats(flattened_z)\n#             print(x.shape)\n#             x = self.dropout(x)\n#             x = self.feat_norm(x)\n\n        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n#         print(\"concat cls tok\", x.shape)\n#         print(self.embed_len)\n        x = x + self.pos_embed[:, -x.size(1):, :]\n#         print(\"add pos embed\", x.shape)\n        # x = self.dropout(x)\n        for block in self.transformer_blocks:\n            x = block(x)\n#             print(\"trans block\", x.shape)\n        # x = self.norm(x)\n        logits = self.cls_head(x[:, 0])\n        \n#         print(\"logits shape\", logits.shape)\n\n        return logits","metadata":{"_uuid":"8e93a9dd-0dbb-4c88-8fd8-3dedd7ef0205","_cell_guid":"51a9fc27-f05c-40ad-9691-f3409fffdb07","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:52.954378Z","iopub.execute_input":"2023-12-11T21:00:52.954791Z","iopub.status.idle":"2023-12-11T21:00:52.975128Z","shell.execute_reply.started":"2023-12-11T21:00:52.954758Z","shell.execute_reply":"2023-12-11T21:00:52.974070Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nimage_size = 32\npatch_size = 4\nin_channels = 3\nembed_dim = 256 # 512\nnum_heads = 4\nmlp_dim = 1024\nnum_layers = 6 # 4\nnum_classes = 10\ndropout = 0.1\nbatch_size = 128 # 256\nmode = \"patch\"","metadata":{"_uuid":"ce4c28ef-24e8-4537-be19-4b7654f22a79","_cell_guid":"7ad2e579-c16b-432b-8b77-145303192f20","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:00:56.705768Z","iopub.execute_input":"2023-12-11T21:00:56.706129Z","iopub.status.idle":"2023-12-11T21:00:56.711211Z","shell.execute_reply.started":"2023-12-11T21:00:56.706099Z","shell.execute_reply":"2023-12-11T21:00:56.710229Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'/kaggle/input/vq-vae-model-pth/vq_vae_model.pth'","metadata":{"_uuid":"b44fecb9-4d5b-426b-8941-900f8f2f1f4f","_cell_guid":"24f0c279-a355-4338-bfa2-57cef7883f2d","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T18:55:17.377403Z","iopub.execute_input":"2023-12-11T18:55:17.377787Z","iopub.status.idle":"2023-12-11T18:55:17.384316Z","shell.execute_reply.started":"2023-12-11T18:55:17.377758Z","shell.execute_reply":"2023-12-11T18:55:17.383230Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vq_dim = 64\npath = '/kaggle/input/vq-vae-model-pth/'\nvqvae_model = VQVAE(in_channels=3, num_embeddings=512, embedding_dim=64, use_ema=False).to(device)\nvqvae_state_dict = torch.load(path+'vq_vae_model.pth')['state_dict']\nvqvae_model.load_state_dict(vqvae_state_dict)\nvqvae_total_params = sum(param.numel() for param in vqvae_model.parameters())\nvqvae_total_params","metadata":{"_uuid":"ae99acad-b690-4b05-8fc9-b7f63298388e","_cell_guid":"9216456f-875e-4cc9-835e-2f7b8e33a7bc","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:01:02.156064Z","iopub.execute_input":"2023-12-11T21:01:02.156433Z","iopub.status.idle":"2023-12-11T21:01:07.366885Z","shell.execute_reply.started":"2023-12-11T21:01:02.156402Z","shell.execute_reply":"2023-12-11T21:01:07.365917Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vqvae_model","metadata":{"_uuid":"f49ff2fb-ee06-4e74-ac64-746fe585d5ed","_cell_guid":"da905cf5-ca39-4d50-9a01-f2a892b9d75d","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:03:17.905837Z","iopub.execute_input":"2023-12-11T21:03:17.906525Z","iopub.status.idle":"2023-12-11T21:03:17.913961Z","shell.execute_reply.started":"2023-12-11T21:03:17.906489Z","shell.execute_reply":"2023-12-11T21:03:17.912868Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"3da67722-0630-4dba-9c70-002fe1983543","_cell_guid":"f9cae82c-c4d5-4beb-bcca-2a9708091607","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"7f48c82f-a3dd-4759-9f3c-090cbbbd7452","_cell_guid":"a09c90aa-cf52-43dc-bd0d-192d41abab9b","trusted":true}},{"cell_type":"code","source":"model = VitWithVQVAE(image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout, \"vqvae\", vq_dim).to(device)\ninput_tensor = torch.randn(1, in_channels, image_size, image_size).to(device)\noutput = model(input_tensor)\nprint(output.shape)","metadata":{"_uuid":"eac7b33a-a639-479e-b7db-6d342b799647","_cell_guid":"3ff4deb3-c05d-4d4e-b6e9-25f22ce95af8","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:02:50.809540Z","iopub.execute_input":"2023-12-11T21:02:50.809922Z","iopub.status.idle":"2023-12-11T21:02:51.433642Z","shell.execute_reply.started":"2023-12-11T21:02:50.809893Z","shell.execute_reply":"2023-12-11T21:02:51.432269Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"a4446740-55b1-4c62-983b-7a2c5060439f","_cell_guid":"aa689a8a-0836-4df1-b06a-c2bb74e2447e","trusted":true}},{"cell_type":"code","source":"model","metadata":{"_uuid":"e9be0a62-c73c-4178-8369-bb8366f8c5d3","_cell_guid":"53f510cf-29df-441d-bd4c-c85874c22a86","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T21:02:13.986064Z","iopub.execute_input":"2023-12-11T21:02:13.987010Z","iopub.status.idle":"2023-12-11T21:02:13.993730Z","shell.execute_reply.started":"2023-12-11T21:02:13.986974Z","shell.execute_reply":"2023-12-11T21:02:13.992752Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vit_total_params = sum(param.numel() for param in model.parameters())\nvit_total_params","metadata":{"_uuid":"fd4edd6b-0892-4cd6-a3d8-f032600fa963","_cell_guid":"bfed760c-31b3-494d-b1b2-bf5b4ad9d24f","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T19:58:21.236152Z","iopub.execute_input":"2023-12-11T19:58:21.237041Z","iopub.status.idle":"2023-12-11T19:58:21.246115Z","shell.execute_reply.started":"2023-12-11T19:58:21.237002Z","shell.execute_reply":"2023-12-11T19:58:21.244727Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vit_total_params = sum(param.numel() for param in model.parameters())\nvit_total_params","metadata":{"_uuid":"8142fdb0-bae5-41da-85dd-4564852afc3c","_cell_guid":"50dff17c-fe6c-432f-9149-f2ac371f7996","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T19:58:39.285395Z","iopub.execute_input":"2023-12-11T19:58:39.286137Z","iopub.status.idle":"2023-12-11T19:58:39.294758Z","shell.execute_reply.started":"2023-12-11T19:58:39.286104Z","shell.execute_reply":"2023-12-11T19:58:39.293517Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import datasets, transforms\n\n# Load the CIFAR-10 dataset\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.Resize(image_size),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntrainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntestset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)","metadata":{"_uuid":"31ae6f58-6857-4ec1-b9d8-6c4d65727fc7","_cell_guid":"fc351c86-8ce2-449e-a642-4527b2db8dec","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T18:56:04.272076Z","iopub.execute_input":"2023-12-11T18:56:04.272732Z","iopub.status.idle":"2023-12-11T18:56:05.973011Z","shell.execute_reply.started":"2023-12-11T18:56:04.272690Z","shell.execute_reply":"2023-12-11T18:56:05.972033Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\n# TODO\nlr = 0.0006 # 0.003\nweight_decay =  0 # 0.0001\nnum_epochs = 50 # 150\noptimizer = torch.optim.Adam(model.parameters(),\n                                lr=lr,\n                                weight_decay=weight_decay)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(trainloader), epochs=num_epochs)","metadata":{"_uuid":"9d3c8e78-0f1c-4028-8d20-e9e62d90a541","_cell_guid":"6ecea7d3-13af-4cde-b6c3-f40a8e13c753","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T20:06:22.832198Z","iopub.execute_input":"2023-12-11T20:06:22.833457Z","iopub.status.idle":"2023-12-11T20:06:22.844402Z","shell.execute_reply.started":"2023-12-11T20:06:22.833421Z","shell.execute_reply":"2023-12-11T20:06:22.842695Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = {'epoch': num_epochs, \n           'lr':lr,\n        'image_size': image_size,\n        'patch_size': patch_size,\n    'in_channels':in_channels,\n'embed_dim':embed_dim,\n'num_heads':num_heads,\n'mlp_dim': mlp_dim,\n'num_layers':num_layers,\n'num_classes':num_classes,\n'batch_size':batch_size,\n'mode':mode,\n'weight_decay':weight_decay,\n'optimizer':'adam',\n'scheduler':'onecyclelr'\n          }\n\n# sweep_configuration = {\n#     \"method\": \"random\",\n#     \"name\": \"sweep\",\n#     \"metric\": {\"goal\": \"maximize\", \"name\": \"val_acc\"},\n#     \"parameters\": {\n#         \"batch_size\": {\"values\": [128, 64]},\n#         \"epochs\": {\"values\": [25, 50]},\n#         \"lr\": {\"max\": 0.01, \"min\": 0.0001},\n#         \"mode\": {\"values\": [\"patch\", \"vqvae\"]}\n#     },\n# }\n# sweep_id = wandb.sweep(sweep=sweep_configuration, project='bagel')\nrun = wandb.init(project=\"bagel\", group='vitvq', config=cfg)\n# run = wandb.init()","metadata":{"_uuid":"d2a8fa61-98e5-44bf-940f-a6eaa8211b8a","_cell_guid":"ace81424-5d3d-4132-b81c-76b9bd9a9c7f","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T20:46:48.788825Z","iopub.execute_input":"2023-12-11T20:46:48.789869Z","iopub.status.idle":"2023-12-11T20:47:24.242642Z","shell.execute_reply.started":"2023-12-11T20:46:48.789826Z","shell.execute_reply":"2023-12-11T20:47:24.241666Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb.agent(sweep_id)\nfrom tqdm import tqdm","metadata":{"_uuid":"4ac3bd72-20d6-4e26-aea2-30addf058234","_cell_guid":"d89fb4dc-7347-4390-9f23-9bb7e62339fc","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T18:56:44.821696Z","iopub.execute_input":"2023-12-11T18:56:44.822277Z","iopub.status.idle":"2023-12-11T18:56:44.826991Z","shell.execute_reply.started":"2023-12-11T18:56:44.822250Z","shell.execute_reply":"2023-12-11T18:56:44.825995Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nbest_val_acc = 0\ntrain_accs = []\ntest_accs = []\nepochs_no_improve = 0\nmax_patience = 20\nearly_stop = False\npbar=tqdm(range(num_epochs))\nfor epoch in pbar:\n    # if not load_pretrained:\n    running_accuracy = 0.0\n    running_loss = 0.0\n    model.train()\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        acc = (outputs.argmax(dim=1) == labels).float().mean()\n        running_accuracy += acc / len(trainloader)\n        running_loss += loss.item() / len(trainloader)\n    \n    train_accs.append(running_accuracy)\n\n    wandb.log({'train_acc':running_accuracy, 'train_loss':loss})\n    # TODO Feel free to modify the training loop youself.\n\n    # Validate the model\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            val_loss = criterion(outputs, labels)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = 100 * correct / total\n    wandb.log({'val_acc':val_acc, 'val_loss':val_loss})\n    pbar.set_postfix({\"Epoch\": epoch+1, \"Train Accuracy\": running_accuracy*100, \"Training Loss\": running_loss, \"Validation Accuracy\": val_acc})\n\n    # Save the best model\n\n    if val_acc > best_val_acc:\n        epochs_no_improve = 0\n        best_val_acc = val_acc\n        torch.save({\n            'epoch': epoch,\n            'model': model.state_dict(),\n            'optimizer': optimizer,\n            'scheduler' : scheduler,\n            'train_acc': train_accs,\n            'test_acc': val_acc\n        },  'best_model.pth')\n\n    else:\n        epochs_no_improve += 1\n\n    if epoch > 100 and epochs_no_improve >= max_patience:\n        print('Early stopping!')\n        early_stop = True\n        break\n    else:\n        continue","metadata":{"_uuid":"f4b23d04-0355-42cc-8fe9-9e137333e763","_cell_guid":"c72537ff-4132-49cf-8ea6-232531819c51","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T20:07:10.439006Z","iopub.execute_input":"2023-12-11T20:07:10.439755Z","iopub.status.idle":"2023-12-11T20:44:14.428961Z","shell.execute_reply.started":"2023-12-11T20:07:10.439719Z","shell.execute_reply":"2023-12-11T20:44:14.427866Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"_uuid":"254ac3bf-b110-40e5-91b9-674185ce737e","_cell_guid":"2986ccdd-e520-4686-beec-36f296c6027c","collapsed":false,"execution":{"iopub.status.busy":"2023-12-11T20:55:16.283300Z","iopub.execute_input":"2023-12-11T20:55:16.283702Z","iopub.status.idle":"2023-12-11T20:55:16.296203Z","shell.execute_reply.started":"2023-12-11T20:55:16.283669Z","shell.execute_reply":"2023-12-11T20:55:16.295042Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"83d04548-f69d-4cd5-a191-317f58e77cc3","_cell_guid":"aadb8108-0f1f-400d-af92-0eaf528017f0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"5897ba88-865d-4db6-bd31-da6a2002ca8b","_cell_guid":"bd07a5ff-d5fa-4d31-8cca-842d019a32e6","execution":{"iopub.status.busy":"2023-12-04T00:47:05.907824Z","iopub.execute_input":"2023-12-04T00:47:05.908314Z","iopub.status.idle":"2023-12-04T00:47:06.324198Z","shell.execute_reply.started":"2023-12-04T00:47:05.908250Z","shell.execute_reply":"2023-12-04T00:47:06.323127Z"},"trusted":true}}]}